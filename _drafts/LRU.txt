http://www.geeksforgeeks.org/implement-lru-cache/
The LRU caching scheme is to remove the least recently used frame when the cache is full and a new page is referenced which is not there in cache
We use two data structures to implement an LRU Cache.
1. A Queue which is implemented using a doubly linked list
The most recently used pages will be near front end and least recently pages will be near rear end.
2. A Hash with page number as key and address of the corresponding queue node as value.
If it is in the memory, we need to detach the node of the list and bring it to the front of the queue.
If the required page is not in the memory, we bring that in memory. In simple words, we add a new node to the front of the queue and update the corresponding node address in the hash.
If the queue is full, i.e. all the frames are full, we remove a node from the rear of queue, and add the new node to the front of queue.



http://blog.csdn.net/luotuo44/article/details/42869325
//将item插入到LRU队列的头部 

所以新鲜item(访问时间新)，要排在不那么新鲜item的前面，所以插入LRU队列的头部是不二选择。

do_item_update函数是先把旧的item从LRU队列中删除，然后再插入到LRU队列中(此时它在LRU队列中排得最前)。

//下面的代码可以看到update操作是耗时的。如果这个item频繁被访问，  
    //那么会导致过多的update，过多的一系列费时操作。此时更新间隔就应运而生  
    //了。如果上一次的访问时间(也可以说是update时间)距离现在(current_time)  
    //还在更新间隔内的，就不更新。超出了才更新。 
    
  前面的代码中很少使用锁。但实际上前述的item操作都是需要加锁的，因为可能多个worker线程同时操作哈希表和LRU队列。之所以很少看到锁是因为他们都使用了包裹函数(如果看过《UNIX网络编程》，这个概念应该不陌生)。在包裹函数中加锁和解锁。
  

  
http://calvin1978.blogcn.com/articles/lru.html
redis
3.0版的改进：现在每次随机五条记录出来，插入到一个长度为十六的按空闲时间排序的队列里，然后把排头的那条删掉，然后再找五条出来，继续尝试插入队列.........嗯，好了一点点吧，起码每次随机多了两条，起码不只在一次随机的五条里面找最久那条，会连同之前的一起做比较......

相比之下，Memcached实现的是再标准不过的LRU算法，专门使用了一个教科书式的双向链表来存储slab内的LRU关系，代码在item.c里，详见memcached源码分析-----LRU队列与item结构体，元素插入时把自己放到列头，删除时把自己的前后两个元素对接起来，更新时先做删除再做插入。

分配内存超限时，很自然就会从LRU的队尾开始清理。







https://github.com/memcached/memcached/blob/c5530027c8ea28674358327ab8212ebaf014c848/items.c


http://www.thinkingyu.com/articles/LIRS/
下面是一些用来证明LRU性能瓶颈的具有代表性的例子：
